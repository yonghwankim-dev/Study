
# A.1 다중 호스트 기반의 도커 스웜 모드 클러스터
## A.1.1 도커 스웜 모드 개요
- 도커 스웜 모드(Docker Swarm mode)는 여러개의 물리적인 서버에 서버 구분 없이 컨테이너를 분산 실행하는 기능입니다.
- 클러스터화된 각 서버의 도커 엔진을 통해 마치 하나의 서버처럼 폴링하여 스웜을 형성합니다.

## A.1.2 도커 스웜 모드 오케스트레이션 도구의 주요 기능
- 여러대의 서버와 여러개의 서비스를 통합 관리하는 작업을 의미
- 스케줄링, 클러스터링, 로깅, 모니터링, 폴링 업데이트, 서비스 디스커버리와 같은 작업 수행

### 도커 엔진과 통합된 다중 서버 클러스터 환경
- 도커 엔진에 포함된 도커 스웜 모드를 이용해서 별도의 오케스트레이션 도구를 설치하지 않아도 컨테이너 애플리케이션 서비스를 배포하고 관리할 수 있습니다.
- 즉, 별도 설치하지 않고 도커만 설치하면 사용할 수 있습니다.

### 역할이 분리된 분산 설계
다중 서버를 클러스터에 합류시키면 모든 도커 스웜 모드의 노드는 각각 다른 역할을 수행하게 됩니다.
- 매니저 노드(manager node)
- 리더 노드(leader node)
- 작업자 노드(worker node)

리더 노드
- 매니저 노드중 하나를 선택하여 매니저 노드 및 작업자 노드 관리

매니저 노드
- 클러스터의 관리 역할로 컨테이너 스케줄링 서비스 및 상태 유지 제공

작업자 노드
- 컨테이너 실행 역할

### 서비스 확장과 원하는 상태 조정
- 도커 스웜 모드에서 서비스 생성할 때 안정적인 서비스를 위해서 중복된 서비스를 배포할 수 있습니다.
- 서비스 장애가 발생하는 경우 서비스를 대체할 새로운 복제본을 자동으로 생성해서 배포합니다.

### 서비스 스케줄링
- 스케줄링 기능은 도커 스웜 모드 클러스터 안에 노드에 작업 단위의 서비스 컨테이너를 배포하는 작업을 말합니다.
- 노드 선택 전략
	- 모든 작업자 노드에 균등 할당하는 spread(분산) 전략
	- 작업자 노드의 자원 사용량을 고려해서 할당하는 binpack(자원대비) 전략
	- 임의의 노드에 할당하는 random(무작위) 전략
- 도커 스웜 모드는 단일 옵션으로 고가용성 분산 알고리즘을 사용합니다.
	- 이 방식은 생성되는 서비스의 복제본을 분산 배포하기 위해서 **현재 복제본이 가장 적은 작업자 노드 중에서 이미 스케줄링된 다른 서비스 컨테이너 수가 가장 적은 작업자 노드를 우선 선택**합니다.

### 로드 밸런싱

# A.2 도커 스웜 모드 클러스터 구성
## A.2.1 도커 스웜 모드 구성을 위한 서버 구성
스웜 모드 클러스터 구성을 위한 EC2 인스턴스를 다음과 같이 구성합니다.
- swarm-manager : ec2 t2.micro
- swarm-worker : ec2 t2.micro

다음과 같이 swarm-manager,  swarm-worker 역할을 수행하는 nemo, nemo-worker 인스턴스를 생성합니다.
![[img/image-673.png]]

nemo ec2 인스턴스의 호스트 네임을 swarm-manager로 설정합니다.
```
sudo hostnamectl set-hostname swarm-manager
cat /etc/hostname
```
![[img/image-672.png]]

nemo-worker 인스턴스에 대해서도 동일하게 호스트네임을 설정합니다.
![[img/image-674.png]]

nemo 호스트의 IPv4 주소와 nemo-worker의 IPv4 주소를 호스트 설정 파일에 추가합니다.
```shell
sudo vim /etc/hosts
```
![[img/image-676.png]]

위 실습을 정리하면 다음과 같습니다.
1. manager와 worker 역할을 수행하는 서버 노드 생성
2. manager와 worker 노드의 호스트 이름 설정
3. manager와 worker의 IPv4 정보를 호스트 파일에 설정하기
4. 두 노드에 docker 프로세스 설치(생략)

## A.2.2 도커 스웜 모드 초기 연결 구성: swarm init와 swarm join

### 도커 스웜 모드 초기화
`docker swarm init [OPTIONS]` 명령어를 이용해서 도커 스웜 초기화를 수행합니다.

nemo 노드에서 우선은 스웜 모드 상태를 조회합니다.
```shell
docker info | grep Swarm
```
![[img/image-677.png]]
실행 결과를 보면 inactive 상태로써 스웜 모드가 비활성화 된 상태입니다.

docker swarm init 명령어를 이용하여 매니저 노드의 스웜을 활성화 시킵니다. 옵션인 `--advertise-addr` 옵션에는 매니저 노드에 접근하기 위한 IPv4를 입력합니다.
```shell
docker swarm init --advertise-addr 172.31.41.31
```
![[img/image-678.png]]

```shell
sudo netstat -nlp | grep dockerd
```
![[img/image-679.png]]
- 도커 스웜 모드의 매니저 노드의 기본 포트 : 2377
- 작업 노드 간의 통신 포트 : 7946/tcp, 7946/udp

### 도커 스웜 모드 작업자 노드 연결
앞절에서 생성한 조인 토큰을 복사한 다음에 작업 노드의 프롬프트에 붙여넣기 합니다. **조인 토큰은 작업자 노드가 매니저 노드와 함께 클러스터에 합류할 수 있도록 하는 비밀 키입니다.**

다음 명령어는 작업자 노드가 매니저 노드(172.31.41.31)에 있는 클러스터에 합류하고자 하는 명령어입니다.
```shell
docker swarm join --token SWMTKN-1-2pp978fs2t7m22tof1lti9xq946ufrqr25uu33h4v6ldtynkhb-b9t9fzommdflv3z36qbzy45s6 172.31.41.31:2377
```
![[img/image-680.png]]
실행 결과를 보면 도커 스웜에 worker로서 조인했다는 것을 볼수 있습니다.


> [!NOTE] 매니저 노드 포트 설정
> 매니저 노드 인스턴스의 2377 포트가 열려있어야 합니다. ec2 인스턴스의 경우에는 보안 그룹의 2377 포트가 열려 있도록 설정해야 합니다.

만약 최초 생성된 조인 키를 분실하는 경우 다음 명령어를 이용하여 조회가 가능합니다.
```shell
docker swarm join-token worker
```
![[img/image-681.png]]
실행 결과를 보면 작업자 노드를 클러스터에 합류시키기 위한 명령어와 조인 토큰 값을 출력합니다.

이번에는 **다중 매니저 노드**를 구성하는 경우에는 매니저 노드 추가에 대한 조인 키도 조회가 가능합니다.
```shell
docker swarm join-token manager
```
![[img/image-683.png]]

노드 연결 구성이 끝나면 매니저 노드에서 작업 노드의 연결을 확인할 수 있습니다.
```shell
docker node ls
```
![[img/image-684.png]]
위 실행 결과를 보면 모든 매니저 노드 및 작업자 노드의 호스트명과 상태, 매니저 노드가 어떤 호스트인지 알수 있습니다.

도커 스웜 모드의 초기화 이후에 스웜 모드 상태를 조회하면 활성화 상태와 노드에 관한 정보 및 오케스트레이션 정보까지 도커 스웜 모드에 대한 세부정보를 확인할 수 있습니다.
```shell
docker info
```
![[img/image-685.png]]
![[img/image-686.png]]
![[img/image-687.png]]

모든 연결 구성이 끝난 후에 도커 스웜 모드 네트워크를 확인합니다.
```shell
docker network ls
```
![[img/image-688.png]]
실행 결과를 보면 docker_gwbridge와 ingress 네트워크가 생긴것을 확인할 수 있습니다. docker_gwbridge 네트워크는 오버레이 네트워크를 개별 도커 메돈의 물리적 네트워크에 연결하는 브리지 네트워크입니다.


> [!NOTE] Ingress Network
> 서비스에 외부 트래픽을 전달하기 위해서 설계된 오버레이 네트워크
> 오버레이 네트워크는 여러 호스트(노드) 간의 통신을 위해 사용되는 가상 네트워크입니다.

> [!NOTE] docker_gwbridge
> Docker Swarm 모드에서 내부 컨테이너 네트워크와 외부 컨테이너 간의 연결을 담당하는 네트워크입니다.
> Swarm 모드에서 `ingress`오버레이 네트워크 컨테이너들이 외부 네트워크(인터넷 포함)와 통신할 수 있도록 해줍니다.

운영 중 노드의 확장을 위해 새로운 토큰이 필요한 경우 다음 명령을 통해 새로 발급할 수 있습니다. `--rotate` 옵션을 통해 새로운 조인 토큰을 생성합니다.
```shell
docker swarm join-token --rotate worker
```

조인 토큰만 새로 발급하는 경우에는 --quiet(-q) 옵션을 사용합니다.
```shell
docker swarm join-token -q worker
```

작업자 노드를 스웜에서 제거하고 싶다면 `leave` 명령어를 사용할 수 있습니다.
```shell
docker swarm leave swarm-worker
```

## A.2.3 도커 스웜 모드 모니터링을 위한 구성
서비스 컨테이너 사용전에 도커 스웜 모드를 모니터링할 수 있는 두가지 도구를 구성해봅니다.
### 도커 스웜 모드 시각화 도구
매니저 노드에서 시각화 도구를 생성해봅니다. 
```shell
docker service create \
--name=viz_swarm \
--publish=7070:8080 \
--constraint=node.role==manager \
--mount=type=bind,src=/var/run/docker.sock,dst=/var/run/docker.sock \
dockersamples/visualizer
```

웹브라우저를 통해서 7070포트로 접속하면 다음과 같은 결과가 출력됩니다.
![[img/image-689.png]]

위와 같은 시각화 도구를 이용해서 모든 서비스를 확인할 수 있습니다.

도커 시각화 도구를 종료합니다.
```shell
docker service rm viz_swarm
```

### 도커 스웜 모드 모니터링을 위한 스웜피트 구성
도커 스웜 피트는 도커 스웜 모드를 관리하기 위해 제공되는 GUI 도구입니다.

도커 스웜피트를 컨테이너로 실행합니다.
```shell
docker run -it --rm \
--name swarmpit-installer \
--volume /var/run/docker.sock:/var/run/docker.sock \
swarmpit/install:1.9
```


실행중에 다음과 같이 애플리케이션 정보를 입력합니다.
![[img/image-690.png]]

docker swarmpit은 도커 스택(docker stack)으로 구동되며, app, agent, db, influxdb의 4개 스택으로 구성되어 있습니다.
```shell
docker stack ps swarmpit
```

app이 다른 워커 노드에 있는 db 컨테이너에 접속이 불가능합니다. 문제 해결이 필요함


# A.3 도커 스웜 모드 워크숍
## A.3.1 도커 스웜 모드 워크숍 1: nginx를 이용한 서비스 컨테이너 배포와 관리
nginx를 2개의 복제본으로 클러스터 노드에 배포한다.
```shell
docker service create \
--name web-alb \
--constraint node.role==worker \
--replicas 2 \
--publish 8001:80 \
nginx
```

```shell
docker service ls
```
![[img/image-691.png]]

```shell
docker service ps web-alb
```
![[img/image-696.png]]

worker 노드에서 컨테이너 확인해봅니다.
```shell
docker ps
```
![[img/image-697.png]]

index.html 파일을 생성합니다.
```shell
vim index.html
```
```html
<h1> hi, docker swarm1 </h1>
```

index2.html 파일을 생성합니다.
```
vim index2.html
```
```
<h1> hi, docker swarm2 </h1>
```
- 첫번째 nginx 컨테이너에는 swarm1을 두번째 nginx 컨테이너는 swarm2를 복사합니다.


워커 노드에서 index.html 파일을 각 컨테이너로 복사합니다.
```shell
docker cp index.html web-alb.1.ztexltkt63r7ign87lr6beavf:/usr/share/nginx/html/index.html
docker cp index2.html web-alb.2.r45x6tlgw42za57djkul11yxr:/usr/share/nginx/html/index.html
```
![[img/image-698.png]]

매니저 노드에서 curl 명령어로 요청을 날려봅니다.
```shell
curl swarm-worker:8001
```
![[img/image-699.png]]
실행 결과를 보면 2개의 nginx 컨테이너에 로드 밸런싱하여 전달하는 것을 볼수 있습니다.

이번에는 nginx 2개로 배포한 서비스를 3개로 확장해보겠습니다.
```shell
docker service scale web-alb=3
```
![[img/image-700.png]]

```shell
docker service ls
```
![[img/image-701.png]]
실행 결과를 보면 3개의 nginx로 확장된 것을 볼수 있습니다.

이번에는 각각의 컨테이너에 대한 정보를 자세히 출력할 수 있도록 해보겠습니다.
```shell
docker service ps web-alb
```
![[img/image-702.png]]
실행 결과를 보면 모두 swarm-worker 노드에서 실행중인 것을 알수 있습니다.

이번에는 반대로 web-alb 서비스를 2개로 축소해보겠습니다.
```shell
docker service scale web-alb=2
```
![[img/image-703.png]]

```shell
docker service ps web-alb
```
![[img/image-704.png]]
실행 결과를 보면 nginx 컨테이너가 2개로 축소된 것을 볼수 있습니다.

스웜 모드 클러스터에는 복제(--replicas) 및 전역 서비스 유형이 있습니다. 위 실습은 복제 모드입니다.

#### 도커 스웜 전역 서비스 옵션
전역(--mode global) 옵션을 사용하면 스웜 모드 스케줄러는 **클러스터에 합류된 모든 노드에 무조건 서비스 컨테이너를 배포**합니다.
```shell
docker service create \
--name global_nginx \
--mode global \
nginx
```
![[img/image-705.png]]


```shell
docker service ls
```
![[img/image-706.png]]
실행 결과를 보면 생성된 컨테이너는 2개로써 각각 매니저 노드와 worker 노드에 생성됩니다.


```shell
docker service ps global_nginx
```
![[img/image-707.png]]
위 실행 결과를 보면 nginx 컨테이너가 각각 swarm-manager와 swarm-worker 노드에서 실행중인 것을 볼수 있습니다.
만약 swarm-worker2를 추가하게 된다면 global_nginx 컨테이너는 swarm-worker2 노드에 자동으로 추가되어 총 3개의 컨테이너가 실행될 것입니다.

## A.3.2 도커 스웜 모드 워크숍 2: 서비스 유지 관리를 위한 기능
기본적으로 대부분의 오케스트레이션 도구는 **장애 복구 기능**을 내장하고 있습니다. 배포된 서비스의 장애 및 노드에 장애가 발생하면 자동으로 복제 수 만큼의 서비스를 맞추어 장애에 대한 **자동 복구**를 수행합니다.
또한 패키지 버전 변경시 **롤링 업데이트** 기능을 수행하면 새 버전의 컨테이너는 한개씩 증가시키고, 이전 버전의 컨테이너는 한개씩 감소시키는 방식으로 버전 업데이트를 수행할 수 있습니다.
만약에 업데이트가 실패하는 경우 재시도하거나 업데이트 중지 등의 옵션으로 선택적으로 실행할 수 있습니다. 또한 잘못된 업데이트를 취소하기 위해서 롤백 기능도 제공합니다.

nginx 서비스를 생성합니다.
```shell
docker service create \
--name web-alb \
--constraint node.role==worker \
--replicas 3 \
--publish 8001:80 \
nginx
```
![[img/image-711.png]]

```shell
docker service ls
```
![[img/image-712.png]]

```shell
docker service ps web-alb
```
![[img/image-713.png]]

위와 같이 3개의 컨테이너를 실행한 상태에서 swarm-worker 노드에서 1개의 nginx 컨테이너를 삭제하여 장애를 유발해보겠습니다.
```shell
docker ps
```
![[img/image-714.png]]
실행 결과를 보면 swarm-worker 노드에서 3개의 nginx 컨테이너가 실행중입니다. 

swarm-worker 노드에서 web-alb.3.xxx nginx 컨테이너를 삭제해보겠습니다.
```shell
docker rm -f web-alb.3.ydozuywhpe2x8rdoam2mxvdru
```
![[img/image-715.png]]

다시 매니저 노드에서 docker service 명령어를 이용하여 web-alb 서비스의 상태를 확인해보겠습니다.
![[img/image-716.png]]
위 실행 결과를 보면 ID가 "ydozuywhpe2x"인 컨테이너는 Shutdown되었고 대신에 새로운 l2kyyxr9dm99라는 새로운 nginx 컨테이너가 실행중인 것을 볼수 있습니다. 이는 web-alb.3에 대한 자동 복구가 이루어진 것이기 때문입니다.

docker service 명령어를 이용하여 web-alb 서비스를 종료합니다.
```shell
docker service rm web-alb
```

#### 실습: 롤링 업데이트 및 롤백
레디스 데이터베이스 컨테이너를 3개로 서비스 배포합니다.
```shell
docker service create \
--name my-database \
--constraint node.role==worker \
--replicas 3 \
redis:6.0-alpine
```
![[img/image-717.png]]

docker service 명령어를 이용하여 실행 상태를 확인합니다.
```shell
docker service ls
```
![[img/image-718.png]]

```shell
docker service ps my-database
```
![[img/image-719.png]]

위와 같은 상태에서 redis 이미지의 버전이 6.0.0-alpine에서 6.2.5-alpine으로 변경해서 다시 배포한다고 가정합니다. 그러면 다음과 같이 명령어를 실행합니다.
```shell
docker service update \
--image redis:6.2.5-alpine \
my-database
```
![[img/image-720.png]]
실행 결과를 보면 redis:6.0-alpine 버전은 차례차례 Shutdown되고 새로운 버전인 redis:6.2.5-alpine 버전으로 배포된 것을 확인할 수 있습니다.

롤링 업데이트 추가 기능
- `--update-failure-action`
	- 설명: 업데이트 도중에 오류가 발생하면 어떤 행동을 취할지 선택하는 옵션입니다.
	- 설정값
		- pause(기본값) : 업데이트를 일시 중지하기
		- continue : 오류가 발생해도 업데이트를 계속 진행하기
		- rollback : 업데이트 실패시 이전 버전으로 롤백하기
- `--update-max-failure-ratio`
	- 설명: 전체 업데이트 대상 중 실패를 허용할 최대 비율을 설정합니다.
	- 기본값 : 0 (하나라도 실패하면 업데이트 중지)
	- 값 범위 : 0.0 ~ 1.0
		- 예를 들어 0.2이면 20%까지 실패를 허용합니다.
- `--update-monitor`
	- 설명: 컨테이너가 업데이트된 후 정상적으로 실행되는지 모니터링하는 시간(초)입니다.
	- 기본값: 0s (0s이면 모터너링을 비활성화합니다.)
	- 설정값: 30s (업데이트된 컨테이너를 30초 동안 모니터링하여 정상 동작하는지 확인합니다.)
- `--update-order`
	- 설명: 컨테이너 업데이트 순서를 설정합니다.
	- 설정값
		- stop-first (기본값) : 기존 컨테이너를 먼저 종료한후에 새 컨테이너를 실행
		- start-first : 새 컨테이너를 먼저 실행한 후 기존 컨테이너 종료
			- 새 컨테이너를 먼저 실행한 후 기존 컨테이너를 종료하여 다운타임을 최소화합니다.
- `--update-parallelism`
	- 설명: 동시에 업데이트할 컨테이너 개수를 설정합니다.
	- 기본값: 1 (한번에 하나씩만 업데이트)
	- 설정값: 1 이상의 자연수

#### 실습: 롤링 업데이트 추가 기능을 이용하여 버전 업데이트
다음 실습은 redis:6.0-alpine 버전을 redis:6.2.5-alpine 버전으로 업데이트합니다.

우선은 현재 실해중인 my-database 서비스의 상태 정보를 조회합니다. 상태 정보에는 생성에 사용한 정보와 업데이트 상태, 롤백 상태 등의 정보를 확인할 수 있습니다.
```shell
docker service inspect --pretty my-database
```
![[img/image-721.png]]
- Parallelism=1 : 한번에 한개씩 업데이트 수행
- On failure=1 : 업데이트 실패시 중지합니다.
- Monitoring Period=5s : 모니터링 간격은 5초로 수행
- Max failure ratio=0 : 하나라도 실패하면 업데이트를 중지합니다.
- Update order=stop-first : 기존 컨테이너를 중지하고 새로운 버전의 컨테이너를 실행합니다.

![[img/image-722.png]]
- Parallelism=1 : 한번에 한개씩 롤백합니다.
- On failure=pause : 롤백에 실패하면 중지합니다.
- Monitoring Period=5s : 모니터링 간격은 5초
- Max failure ratio: 하나라도 롤백에 실패하면 롤백을 중지합니다.
- Rollback order=stop-first : 기존 컨테이너를 중지하고 롤백할 컨테이너를 실행합니다.

my-database2라는 이름으로 redis:6.0-alpine 컨테이너를 4개 배포하는 서비스를 실행합니다. 이번에는 롤링 업데이트의 추가 기능을 추가하여 실행합니다.
```shell
docker service create \
--name my-database2 \
--replicas 4 \
--update-delay 10s \
--update-parallelism 2 \
redis:6.0-alpine
```
![[img/image-723.png]]

my-database2 서비스의 변경된 정보를 조회해봅니다.
```shell
docker service inspect --pretty my-database2
```
![[img/image-724.png]]
![[img/image-725.png]]
위 실행 결과를 보면 UpdateConfig.Parallelism 설정이 2인 것을 볼수 있고, 이는 업데이트시 컨테이너를 2개씩 병렬적으로 업데이트한다는 것을 알수 있습니다. 
UpdateConfig.Delay 옵션을 보면 10s인 것을 볼수 있는데, 이는 각 컨테이너 업데이트 사이의 지연 시간을 10초로 설정한 것입니다.

위와 같은 설정 상태에서 my-database2 서비스의 redis 이미지 버전을 redis:6.0.0-alpine에서 redis:6.2.5-alpine으로 변경하여 업데이트가 2개씩 수행되고 각 업데이트 사이가 10초의 지연이 있는지 확인합니다.
```shell
docker service update \
--image redis:6.2.5-alpine \
my-database2
```
![[img/image-726.png]]
실행 결과를 보면 2개씩 업데이트를 수행하고 10초의 지연을 가지는 것을 볼수 있습니다.

업데이트가 완료된 my-database2 서비스의 실행 상태를 확인해보겠습니다.
```shell
docker service ps my-database2
```
![[img/image-727.png]]
실행 상태를 보면 redis:6.0-alpine 버전은 Shutdown되었고 새로운 버전의 redis:6.2.5-alpine 버전으로 배포된 것을 볼 수 있습니다.

#### 실습: 도커 스웜 롤백을 이용하여 이전 버전으로 되돌리기
`docker service rollback` 명령어를 이용하여 이전 버전으로 되돌려보겠습니다.
```shell
docker service rollback my-database2
```
![[img/image-728.png]]

롤백을 완료하면 my-database2의 실행 상태를 확인합니다.
```shell
docker service ps my-database2
```
![[img/image-729.png]]
실행 결과를 보면 redis:6.2.5-alpine -> redis:6.0-alpine 순서로 다시 롤백한 것을 확인 할 수 있습니다.


> [!NOTE] 롤백 시점은 어느 시점까지 가능한가?
> 기본적으로 5 버전 이전까지 가능합니다. docker info 명령어를 이용하여 조회한 정보 중 Task History Retention Limit의 값이 5인 것을 확인할 수 있습니다.

```shell
docker info | grep 'Task History Retention Limit'
```
![[img/image-730.png]]
위 실행 결과를 보면 Task History Retention Limit의 값이 5인 것을 확인할 수 있습니다.

만약 본인의 서비스가 버전 업데이트가 자주 발생한다면 그 보관 수를 늘릴 수 있습니다.
```shell
docker swarm update --task-history-limit 10
```
![[img/image-731.png]]

다시 docker 정보를 확인합니다.
```
docker info | grep 'Task History Retention Limit'
```
![[img/image-732.png]]
실행 결과를 보면 성공적으로 10개로 변경되었습니다.

#### 실습: 도커 스웜 드레인(Drain) 모드
만약 특정 노드에서 **하드웨어 교체 및 정기 유지보수 작업 등의 유지 관리**를 하기 위해서 **업데이트 작업에서 제외해야** 하는 경우가 있습니다. 도커 스웜 모드에서는 이러한 작업을 드레인(Drain, 배출하다) 모드라고 합니다. 드레인의 의미처럼 해당 노드를 현재 클러스터에서 임시로 배제합니다.

```shell
docker service create \
--name my-web \
--replicas 3 \
nginx:1.19
```
![[img/image-733.png]]

my-web 서비스의 실행 상태를 확인합니다.
```shell
docker service ps my-web
```
![[img/image-734.png]]
실행 상태를 보면 my-web.1, my-web.3 컨테이너는 swarm-worker 노드에서 실행 중인 것을 볼수 있습니다.

nginx:1.19를 nginx:1.21 버전으로 업데이트를 수행하려고 합니다. 그러나 swarm-manager는 서버에 문제가 있어서 업데이트에 제외시키고자 합니다.
```shell
docker node ls
```
![[img/image-735.png]]

```shell
docker node update --availability drain swarm-manager
```
![[img/image-736.png]]
실행 결과를 보면 swarm-manager의 availability 상태를 보면 Drain 상태인 것을 볼 수 있습니다.

이번에는 my-web 서비스의 실행 상태를 확인해봅니다.
```shell
docker service ps my-web
```
![[img/image-737.png]]
실행 결과를 보면 swarm-manager에서 동작하던 my-web.2이 swarm-worker 노드에서 실행하고 있는 것을 볼수 있습니다. **이는 다른 노드로 해당 작업이 이전된 것입니다.**

이제 현재 실행중인 my-web 서비스에 업데이트를 수행합니다.
```shell
docker service update \
--image nginx:1.21 \
my-web
```

업데이트를 수행한 다음에 my-web 서비스의 실행 상태를 확인해봅니다.
```shell
docker service ps my-web
```
![[img/image-738.png]]
업데이트가 성공적으로 변경된 것을 볼수 있습니다.

다시 swarm-manager 노드의 availability를 drain 모드에서 active 모드로 변경합니다. 하지만 **active 모드로 변경한다고 해서 기존 swarm-worker에서 작동하는 my-web.2 컨테이너가 다시 swarm-manager로 이전되지는 않습니다.**
```shell
docker node update --availability active swarm-manager
```
![[img/image-739.png]]

my-web 서비스에서 컨테이너 개수를 1개로 축소시켜서 재분배를 수행합니다.
```shell
docker service scale my-web=1
docker service ps my-web
```

![[img/image-740.png]]

다시 컨테이너 개수를 3개로 업데이트하여 재분배합니다.
```shell
docker service scale my-web=3
docker service ps my-web
```
![[img/image-742.png]]
실행 결과를 보면 my-web.2 컨테이너가 다시 swarm-manager에서 동작하는 것을 볼수 있습니다. 이는 **swarm-manager가 drain 모드에서 active 모드로 변경하였기 때문입니다.**

## A.3.3 도커 스웜 모드 워크숍 3: 도커 스웜 스택을 이용한 애플리케이션 서비스 생성
**도커 스웜 스택(swarm stack)은 클러스터 환경에서 여러 서비스를 묶어서 생성하는 하나의 스택 공간을 생성하는 것입니다.** 도커 스택은 분산 애플리케이션의 최상위 계층입니다.

#### 실습: 도커 스웜 스택을 이용하기
스택 구성은 다음과 같습니다.
- 프론트 엔드: 3000번 포트, 웹 애플리케이션 호스팅
- 백엔드 : 8000번 포트, API 호스팅
- 데이터베이스 : 27017번 포트, 데이터베이스를 호스팅
프론트 엔드 레이어는 외부 연결과 통신하기 위한 포트를 노출합니다. 하지만 데이터베이스 레이어와는 직접적으로 연결하지 않고 백엔드 레이어을 통해서 통신하도록 구성됩니다. 백엔드 레이어만이 데이터베이스 레이어와만 통신합니다.

우선은 스택에서 사용할 클러스터 네트워크인 오버레이를 생성합니다.
```shell
docker network create --driver overlay dailylog-net
```
![[img/image-743.png]]

dailylog-net 네트워크 생성을 확인합니다.
```shell
docker network ls
```
![[img/image-744.png]]

dailylog 라는 이름의 디렉토리를 생성하고 해당 디렉토리로 이동합니다.
```shell
mkdir dailylog
cd dailylog
```

도커 스택을 구성하기 위해서 yaml 파일을 생성하고 작성합니다.
```shell
vim daily-log.yaml
```

daily-log.yaml
```yaml
version: '3.9'
services:
  mongodb:
    image: dbgurum/dailylog:db_1.0 # mongo db를 이용한 이미지
    ports:
      - "17017:27017"
    networks:
      - dailylog-net
    deploy:
      placement:
        constraints: [node.role != manager]
      restart_policy:
        condition: on-failure
        max_attempts: 3
        delay: 10s
        window: 120s
  frontend:
    image: dbgurum/dailylog:front_1.0
    ports:
      - "300:8000"
    networks:
      - dailylog-net
    environment:
      - PORT=8000
      - DAILYLOG_API_ADDR=backend:8000
    deploy:
      replicas: 2
      placement:
        constraints: [node.role != manager]
      restart_policy:
        condition: on-failure
        max_attempts: 3
        delay: 10s
        window: 120s
    depends_on:
      - backend
  backend:
    image: dbgurum/dailylog:back_1.0
    networks:
      - dailylog-net
    environment:
      - PORT=8000
      - DAILYLOG_DB_ARR=mongodb:27017
    deploy:
      replicas: 2
      placement:
        constraints: [node.role != manager]
      restart_policy:
        condition: on-failure
        max_attempts: 3
        delay: 10s
        window: 120s
    depends_on:
      - mongodb
networks:
  dailylog-net:
    external: true # 스택 코드 외부에서 생성한 오버레이 네트워크임을 의미
```


> [!NOTE] 오버레이 네트워크(overlay network)
> 도커 스웜이나 쿠버네티스와 같은 오케스트레이션 환경에서 여러 호스트(Node) 간에 컨테이너들이 안전하게 통신할 수 있도록 구성된 가상 네트워크입니다.

위에서 작성한 도커 스택 코드를 이용하여 dailylog 서비스를 실행합니다.
```shell
docker stack deploy --compose-file daily-log.yaml dailylog
```
![[img/image-745.png]]

dailylog 서비스 목록을 확인합니다.
```shell
docker service ls
```
![[img/image-746.png]]

docker stack 명령어를 이용해서 dailylog 서비스의 상세한 정보를 조회합니다.
```shell
docker stack ps dailylog
```
![[img/image-747.png]]

웹 브라우저를 이용한여 3000번 포트로 접속해봅니다. 해당 결과는 워커 노드의 IPv4주소:3000 포트로 접속한 것입니다.
![[img/image-748.png]]

위 화면에서 제목과 내용을 입력하면 다음과 같이 밑에 데일리 로그가 출력됩니다.
![[img/image-749.png]]

