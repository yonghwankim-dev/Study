
- [[#7.1 컨트롤러란?|7.1 컨트롤러란?]]
- [[#7.2 ReplicaSet|7.2 ReplicaSet]]
- [[#7.3 Deployment|7.3 Deployment]]
	- [[#7.3 Deployment#7.3.1 쿠버네티스 리소스 컨셉|7.3.1 쿠버네티스 리소스 컨셉]]
- [[#7.4 StatefulSet|7.4 StatefulSet]]
- [[#7.5 DaemonSet|7.5 DaemonSet]]
- [[#7.6 Job & CronJob|7.6 Job & CronJob]]
	- [[#7.6 Job & CronJob#7.6.2 CronJob|7.6.2 CronJob]]
- [[#7.7 마치며|7.7 마치며]]


## 7.1 컨트롤러란?
- 쿠버네티스의 컨트롤러는 클러스터 내의 현재 배포 상태를 사용자가 원하는 배포 상태(바라는 상태)로 변경시켜주는 컴포넌트입니다.
- 컨트롤러의 대표적인 예시로 에어컨 시스템이 있습니다. 현재 온도가 20도이고 사용자가 23도를 원한다면 에어컨은 시간이 지날수록 점점 20도에서 23도로 온도를 높히게 됩니다.
- 쿠버네티스 컨트롤러는 control-loop라는 루프를 지속적으로 돌면서 특정 리소스에 대해 관찰합니다.
- 이번 장에서는 **ReplicaSet, Deployment, Job & CronJob, DaemonSet, StatefulSet** 등과 같은 컨트롤러에 대해서 살펴봅니다.
	- 위 컨트롤러들은 kube-controller-manager 컴포넌트 안에서 동작합니다.
	- 쿠버네티스에서는 내장 컨트롤러 외에 사용자 정의 컨트롤러도 생성할 수 있습니다.

## 7.2 ReplicaSet
- ReplicaSet 리소스는 **Pod를 복제하는 역할**을 수행합니다.
	- Pod를 복제하여 안정적인 서비스를 운영하기 위해 가용성을 높이는 역할을 수행합니다.
- 한개의 Pod에 문제가 생기더라도 다른 복제된 Pod를 이용하여 동일한 서비스를 제공할 수 있습니다.

다음 파일은 RelicaSet yaml 정의서입니다.
```yaml
# myreplicaset.yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: myreplicaset
spec:
  replicas: 2 # 유지할 Pod 개수 설정
  selector:
    matchLabels:
      run: nginx-rs
  template:
    metadata:
      labels:
        run: nginx-rs
    spec:
      containers:
      - name: nginx
        image: nginx
```
- spec.replicas: 복제할 Pod의 개수를 정의합니다.
- selector.matchLabels: 복제 개수를 유지할 Pod를 선택합니다. 예제에서는 레이블이 run=nginx-rs을 가진 Pod를 대상으로 2개를 유지합니다.
- template: 복제할 Pod를 정의합니다.

ReplicaSet 리소스를 생성하고 리스트를 조회해봅니다.
```shell
kubectl apply -f myreplicaset.yaml
```
![[image-204.png]]

ReplicaSet 리소스를 확인해봅니다.
```shell
kubectl get replicaset
```
![[image-205.png]]
- DESIRED : 사용자가 원하는 POD 개수 상태
- CURRENT : 현재 배포된 POD 개수 상태
- READY : 생성된 Pod 중 준비가 완료된 Pod 개수를 의미합니다.

```shell
kubectl get pod
```
![[image-206.png]]
실행 결과를 보면 Pod의 이름이 myreplicaset-xxx 형태인 것을 볼수 있습니다. 이것은 myreplicaset 리소스가 생성한 Pod입니다.'
**ReplicaSet에서는 복제와 유지의 기능만 담당할 뿐, 실제 프로세스 실행은 Pod 리소스를 활용하여 컨테이너를 실행합니다.**

#### 스케일 업/아웃 명령어
`kubectl scale` 명령어를 이용해서 ReplicaSet의 선언된 Pod의 개수를 수정할 수 있습니다.

명령어 형식
```shell
kubectl scale rs --replicas {NUMBER} {REPLICASET_NAME}
```

myreplicaset ReplicaSet 리소스를 대상으로 Pod의 개수를 4개로 스케일 아웃해보겠습니다.
```shell
kubectl scale rs --replicas 4 myreplicaset
```
![[image-207.png]]

ReplicaSet 상태를 확인해봅니다.
```shell
kubectl get rs
```
![[image-208.png]]
실행 결과를 보면 바라는 상태가 4개로 변경된 것을 볼수 있습니다.

그러면 실제로 Pod도 4개로 증가되었는지 확인해보겠습니다.
```shell
kubectl get pod 
```
![[image-209.png]]

이번에는 Pod 4개중 1개를 일부로 제거해보겠습니다.
```shell
kubectl delete pod myreplicaset-6bxnk
```
![[image-210.png]]

Pod를 삭제한 다음에 Pod의 상태를 확인해보겠습니다.
```shell
kubectl get pod
```
![[image-211.png]]
실행 결과를 보면 Pod의 개수가 다시 4개로 유지된 것을 볼수 있습니다. 이는 ReplicaSet이 현재 Pod의 개수와 바라는 상태의 Pod 개수를 비교를 하다가 틀리면 Pod를 생성하여 개수를 맞추게 됩니다.

ReplicaSet 리소스를 정리합니다.
```shell
kubectl delete rs --all
```

## 7.3 Deployment
- Deployment 리소스는 애플리케이션 업데이트 및 배포에 특화된 기능을 수행하는 리소스입니다.
- Deployment 리소스는 다음과 같은 역할을 수행합니다.
	- 롤링 업데이트 지원
	- 업데이트 히스토리를 저장하고 롤백할 수 있는 기능 지원
	- ReplicaSet과 동일하게 Pod의 개수를 증가시킬 수 있음
	- 배포 상태 확인 가능합니다.

직접 Deployment 리소스를 생성하여 기능들을 확인해보겠습니다.
```yaml
# mydeploy.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mydeploy
spec:
  replicas: 10
  selector:
    matchLabels:
      run: nginx
  strategy: # Deployment 전략 설정
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 25%
      maxSurge: 25%
  template:
    metadata:
      labels:
        run: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
```
- replicas : ReplicaSet과 동일하게 유지할 Pod의 개수를 정의합니다.
- matchLabels : 라벨링 시스템을 이용해서 라벨이 `run=nginx`인 Pod를 대상으로 배포를 수행합니다.
- strategy.type: 배포 전략 종류를 선택합니다.
	- 배포 전략 종류
		- RollingUpdate, Recreate
	- RollingUpdate
		- 점진적으로 업데이트를 수행합니다.
	- Recreate
		- 일시적으로 전체 Pod가 삭제되고 새로운 Pod가 전체 생성됩니다.
	- 개발중인 Deployment를 제외하고는 대부분의 경우 RollingUpdate를 사용합니다.
- strategy.rollingUpdate.maxUnavailable : 최대 중단 Pod 허용 개수(또는 비율)를 지정합니다.
	- 예를 들어 총 10개의 replica에 maxUnvailable의 비율이 25%이면 약 2개(소수점 내림)의 예전 Pod가 RollingUpdate 중에 일시 중단될 수 있다는 것을 의미합니다.
	- 개수로 지정시 2개를 설정하면 25%로 지정한 것과 동일한 효과가 발생합니다.
- strategy,rollingUpdate.maxSurge : 최대 초과 Pod 허용 개수(또는 비율)을 지정합니다.
	- 예를 들어 총 10개의 replica에 maxSurge 비율이 25%이면 약 3개(소수점 올림)의 새로운 Pod가 초과하여 최대 13개까지 생성될 수 잇습니다.
	- 개수로 지정시 3개를 설정하면 25%로 지정한 것과 동일한 효과 발생합니다.
- template : 복제할 Pod를 정의합니다.
	- Pod의 spec과 동일합니다.

mydeploy.yaml 파일을 이용하여 Deployment 리소스를 생성합니다.
```shell
kubectl apply --record -f mydeploy.yaml
```
- `--record` : 업데이트 히스토리에 추가하는 옵션입니다. 추후 롤백 같은 기능에 사용됩니다.
![[image-212.png]]

생성한 Deployment 리소스를 확인합니다.
```shell
kubectl get deployment
```
![[image-213.png]]

ReplicaSet 생성을 확인합니다.
```shell
kubectl get rs
```
![[image-214.png]]

생성된 Pod 정보를 확인합니다.
```shell
kubectl get pod
```
![[image-215.png]]
- mydeploy Deployment 리소스가 ReplicaSet 리소스를 생성했고(mydeplooy-8d55fcd7d), 해당 ReplicaSet이 Pod 복제본들을 생성했습니다.
- Deployment, ReplicaSet, Pod 역할을 정리하면 다음과 같습니다.
	- Deployment: 배포 담당
	- ReplicaSet: 복제 담당
	- Pod: 컨테이너 실행 담당

이번에는 새로운 버전의 컨테이너로 업데이트 시 롤링 업데이트가 되는지 확인해보겠습니다.
`kubectl set image` 명령어를 사용하여 nginx 버전을 1.7.9에서 1.9.1로 업그레이드 해보겠습니다.
```shell
kubectl set image deployment mydeploy nginx=nginx:1.9.1 --record
```
![[image-216.png]]
위 실행 결과를 보면 Pod가 몇개씩 업데이트를 수행하는 것을 볼수 있습니다.

특정 Pod의 이미지 tag 정보를 확인해봅니다.
```shell
kubectl get pod mydeploy-7db9fb5448-28k8g -o yaml | grep "image: nginx"
```
![[image-217.png]]
실행 결과를 보면 1.9.1로 성공적으로 변경된 것을 볼수 있습니다.

#### Deployment 롤백 기능
새로운 애플리케이션 배포시 의도치 않은 문제가 발생할 수 있습니다. 이러한 경우에 Deployment 리소스의 롤백 기능을 이용하여 배포를 롤백할 수 있습니다.

예를 들어 의도적으로 존재하지 않는 nginx 버전으로 이미지를 수정하고 배포 상태를 확인해보겠습니다.
```shell
kubectl set image deployment mydeploy nginx=nginx:1.9.21 --record
```
![[image-218.png]]

이미지 버전을 수정한 다음에 Pod의 상태를 확인해봅니다.
```shell
kubectl get pod
```
![[image-219.png]]
실행 결과를 보면 5개의 Pod가 에러가 발생하였고 maxUnvailable 값으로 인해서 최소 8개의 Running 중인 Pod를 유지하고 있고, 새로 생성된 Pod가 동작하지 않기 때문에 더 이상 기존 Pod를 삭제하지 않고 있습니다.
즉, 10개의 Pod에서 2개의 Pod가 에러가 발생하고 나머지 Pod는 업데이트하지 않고 실행합니다. 그리고 maxSurge에 의해서 3개의 Pod를 업데이트를 시도했지만, 이 Pod 또한 업데이트 실패하여 총 5개의 에러 Pod가 발생한 것입니다.

위와 같이 에러가 발생한 배포에 대해서 롤백하기 위해서 `kubectl rollout` 명령어를 사용하여 히스토리를 확인하고 롤백합니다.
```shell
kubectl rollout history deployment mydeploy
```
![[image-220.png]]
실행 결과를 보면 REVISION=3이 가장 최근에 배포한 버전입니다.

nginx:1.9.1 버전으로 롤백을 시도합니다.
```shell
kubectl rollout undo deployment mydeploy
```
![[image-221.png]]

다시 mydeploy Deployment의 히스토리를 확인하니다.
```shell
kubectl rollout history deployment mydeploy
```
![[image-222.png]]
실행 결과를 보면 최신 버전이 다시 nginx:1.9.1로 변경된 것을 볼수 있습니다.

Pod 상태를 확인하고, 버전을 확인합니다.
```shell
kubectl get pod
```
![[image-223.png]]

```shell
kubectl get deployment mydeploy -o yaml | grep image
```
![[image-224.png]]

직접 배포 버전(revision)을 명시하여 곧바로 해당 버전으로 롤백할 수 있습니다.
```shell
# 1.9.1 -> 1.7.9(revision 1)로 롤백
kubectl rollout undo deployment mydeploy --to-revision=1
```
![[image-225.png]]

#### Deployment 리소스의 Pod 개수 조절하기
명령어 형식
```shell
kubectl scale deployment {DEPLOYMENT_NAME} --replicas {NUMBER} 
```

예를 들어 mydeploy Deployment의 replicas를 기존 10에서 5로 설정해보겠습니다.
```shell
kubectl scale deployment mydeploy --replicas=5
```
![[image-226.png]]

5개로 변경되었다면, Pod 상태를 확인해봅니다.
```shell
kubectl get pod
```
![[image-227.png]]

다시 Pod의 개수를 10개로 설정해보겠습니다.
```shell
kubectl scale deployment mydeploy --replicas=10
```
![[image-228.png]]

10개의 Pod가 되었는지 확인해봅니다.
```shell
kubectl get pod
```
![[image-229.png]]

마지막으로 Deployment 리소스도 `edit` 명령어로 직접 yaml 파일을 수정할 수 있습니다.
```shell
kubectl edit deploy mydeploy
```
![[image-230.png]]
위 편집기 창에서 spec.replicas 설정을 3으로 수정해보겠습니다.

![[image-231.png]]

```shell
kubectl get pod
```
![[image-232.png]]
실행 결과를 보면 Pod의 개수가 10개에서 3개로 변경된 것을 볼수 있습니다.

#### Deployment 리소스 정리
```shell
kubectl delete deployment --all
```


### 7.3.1 쿠버네티스 리소스 컨셉
- 쿠버네티스는 작은 기능을 수행하는 다양한 리소스들을 조합할 수 있습니다.
- 대표적으로 Pod, Service, ReplicaSet, Deployment 리소스가 있습니다.
- 위에서 소개한 리소스들을 조합하여 큰 애플리케이션을 구축합니다.

## 7.4 StatefulSet
- StatefulSet은 Stateful한 Pod를 생성해야 하는 경우 사용합니다.
- Deployment나 ReplicaSet과 다르게 복제된 Pod가 완벽히 동일하지 않고 **순서에 따라 고유한 역할**을 가집니다.
- StatefulSet은 **상태 정보를 저장하는 애플리케이션**에서 사용하는 리소스입니다.
- StatefulSet은 Deployment와 유사하게 relica 개수를 관리하지만 StatefulSet은 각 Pod의 순서와 고유성을 보장합니다.
- StatefulSet 사용 사례
	- 고유 Pod 식별자가 필요한 경우
	- 명시적으로 Pod마다 저장소가 지정되어야 하는 경우
		- 예를 들어 1번 디스크는 pod1로, 2번 디스크는 pod2로 지정
	- Pod 끼리의 순서에 민감한 애플리케이션
		- 예를 들어, 1천번재 생성되는 것이 master Pod, 나머지 slave Pod 등으로 구분하는 경우
	- 애플리케이션이 순서대로 업데이트 되어야 하는 경우


> [!NOTE] Deployment에서는 상태를 저장할 수 없나요?
> Deployment 리소스에서도 DB를 연결하거나 외부 스토리지를 사용하는 경우 상태를 저장할 수 있습니다. Pod 내부적으로 상태를 유지해야 하는 경우 StatefulSet을 사용합니다.(Pod로 DB를 구축하는 경우)

다음 yaml 정의서를 작성하여 StatefulSet을 생성해봅니다.
```yaml
# mysts.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysts
spec:
  serviceName: mysts
  replicas: 3
  selector:
    matchLabels:
      run: nginx
  template:
    metadata:
      labels:
        run: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        volumeMounts:
        - name: nginx-vol
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
  - metadata:
      name: nginx-vol
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 1Gi
---
apiVersion: v1
kind: Service
metadata:
  name: mysts
spec:
  clusterIP: None
  ports:
  - port: 8080
    protocol: TCP
    targetPort: 80
  selector:
    run: nginx
```
- spec.serviceName : StatefulSet과 연결할 Service 리소스 이름을 설정합니다.
- selector.matchLabels: StatefulSet도 라벨링 시스템을 이용해서 Pod를 선택합니다.
- template : 복제할 Pod를 정의합니다.
- volumeClaimTemplates : 동적으로 볼륨을 생성하는 property입니다.

위 파일을 기반으로 StatefulSet 리소스를 생성합니다.
```shell
kubectl apply -f mysts.yaml
```
![[image-233.png]]

생성한 StatefulSet 리소스를 확인합니다.
```shell
kubectl get statefulset
```
![[image-234.png]]

mysts StatefulSet 리소스에 의해서 생성된 Pod도 확인합니다.
```shell
kubectl get pod
```
![[image-235.png]]
실행 결과를 보면 Pod 리소스 3개가 생성되어 있습니다. Deployment 리소스와는 다르게 mysts-0과 같이 명시적으로 Pod 순서가 적힌 식별자가 Pod 이름으로 생성되어 있습니다.
또한, Pod 생성 순서가 식별자 순서에 따라서 순차적으로 재생됩니다. 즉, 0번째, 1번째, 2번째 순서로 생성 및 실행됩니다. 

이번에는 각 Pod의 호스트 이름을 출력하여 호스트 이름으로도 구분이 되는지 확인해봅니다.
```shell
kubectl exec mysts-0 -- hostname
kubectl exec mysts-1 -- hostname
kubectl exec mysts-2 -- hostname
```
![[image-236.png]]
실행 결과를 보면 hostname 또한 Pod 이름과 동일한 것을 볼수 있습니다. 즉, 각 Pod는 호스트 이름으로도 구분이 가능합니다.

이번에는 nginx의 html 디렉토리에 각각의 호스트 이름을 저장하고 호출해보겠습니다.
```shell
kubectl exec mysts-0 -- sh -c \
  'echo "$(hostname)" > /usr/share/nginx/html/index.html'
```
```shell
kubectl exec mysts-1 -- sh -c \
  'echo "$(hostname)" > /usr/share/nginx/html/index.html'
```

```shell
kubectl exec mysts-0 -- curl -s http://localhost
kubectl exec mysts-1 -- curl -s http://localhost
```
![[image-237.png]]
![[image-238.png]]
실행 결과를 보면 각각의 호스트 이름을 출력하는 것을 볼수 있습니다. 이 예제를 통해서 알수 있는 사실은 **StatefulSet의 Pod들이 동일한 저장소를 바라보는 것이 아닌 각자의 볼륨을 사용하는 것을 알수 있습니다.**

마지막으로 StatefulSet의 replica 개수를 줄여보겠습니다.
```shell
kubectl scale sts mysts --replicas=0
kubectl get pod
```
![[image-239.png]]
- 삭제될때는 식별자의 역순으로 Pod가 삭제됩니다.
- StatefulSet 리소스는 Pod의 순서에 따라서 다른 역할을 맡거나 Pod 생성 순서를 보장받아야 하는 경우 사용하는 리소스입니다.

#### 리소스 정리
```shell
kubectl delete sts mysts
kubectl delete svc mysts
kubectl delete pvc --all
```

## 7.5 DaemonSet
- DaemonSet 리소스는 **모든 노드에 동일한 Pod를 실행하고자 할 때 사용하는 리소스**입니다.
- DaemonSet 리소스는 리소스 모니터링, 로그 수집기 등과 같이 모든 노드에 동일한 Pod가 위치하면서 노드에 관한 정보를 추출할 때 사용합니다.

다음은 모든 노드의 로그 정보를 추출하는 fluentd DaemonSet 예시입니다.
```yaml
# fluentd.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd
spec:
  selector:
    matchLabels:
      name: fluentd
  template:
    metadata:
      labels:
        name: fluentd
    spec:
      containers:
      - name: fluentd
        image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2
        volumeMounts:
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
      volumes:
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
```

fluentd DaemonSet 리소스를 생성합니다.
```shell
kubectl apply -f fluentd.yaml
```
![[image-240.png]]

생성한 DaemonSet 리소스를 조회합니다.
```shell
kubectl get daemonset
```
![[image-241.png]]
실행 결과를 보면 2개의 Pod가 생성된 것을 볼수 있습니다.

생성한 Pod 정보를 조회합니다.
```shell
kubectl get pod
```
![[image-242.png]]

실행중인 fluentd Pod의 로그 정보를 확인합니다.
```shell
kubectl logs fluentd-7hqmf
```
![[image-243.png]]

DaemonSet 리소스는 모든 노드에 항상 동일한 작업을 수행해야 하는 경우 사용하는 리소스입니다.

**DaemonSet 장점**
- 클러스터에 노드를 새롭게 추가 시, 다로 작업을 수행하지 않아도 신규로 편입된 노드에 자동으로 필요한 Pod들이 생성됩니다.

**DaemonSet 리소스 정리**
```shell
kubectl delete ds --all
```

## 7.6 Job & CronJob
- Job 리소스는 일반 Pod 처럼 항상 실행중인 서비스 프로세스가 아닌 한번 실행하고 완료가 되는 일괄 처리 프로세스입니다.
	- 예를 들어 Job 리소스로 머신 러닝 학습이 있습니다.

다음은 머신 러닝 학습하는 파이썬 코드입니다.
```python
# train.py
import os, sys, json
import keras
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.optimizers import RMSprop

#####################
# parameters
#####################
epochs = int(sys.argv[1])
activate = sys.argv[2]
dropout = float(sys.argv[3])
print(sys.argv)
#####################

batch_size, num_classes, hidden = (128, 10, 512)
loss_func = "categorical_crossentropy"
opt = RMSprop()

# preprocess
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train = x_train.reshape(60000, 784)
x_test = x_test.reshape(10000, 784)
x_train = x_train.astype('float32') / 255
x_test = x_test.astype('float32') / 255

# convert class vectors to binary class matrices
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)

# build model
model = Sequential()
model.add(Dense(hidden, activation='relu', input_shape=(784,)))
model.add(Dropout(dropout))
model.add(Dense(num_classes, activation=activate))
model.summary()

model.compile(loss=loss_func, optimizer=opt, metrics=['accuracy'])

# train
history = model.fit(x_train, y_train, batch_size=batch_size, 
        epochs=epochs, validation_data=(x_test, y_test))

score = model.evaluate(x_test, y_test, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])
```

Job 리소스에서 사용할 이미지를 생성합니다.
```Dockerfile
# Dockerfile
FROM python:3.6.8-stretch
RUN pip install tensorflow==1.5 keras==2.0.8 h5py==2.7.1
COPY train.py .
ENTRYPOINT ["python", "train.py"]
```

이미지를 빌드하고 도커 원격 이미지 저장소에 이미지를 업로드합니다.
```shell
docker build -t nemo1107/train .
docker login
docker push nemo1107/train
```

마지막으로 Job 리소스를 작성합니다.
```yaml
# job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: myjob
spec:
  template:
    spec:
      restartPolicy: Never
      containers:
      - name: ml
        image: nemo1107/train
        args: ['3', 'softmax', '0.5']
  backoffLimit: 2
```
- backoffLimit : 재시도 횟수, 총 2번의 재시도 이후에도 실패하면 최종적으로 실패로 기록됨

job 리소스를 생성합니다.
```shell
kubectl apply -f job.yaml
```
![[image-244.png]]

job 리소스를 상태 조회합니다.
```shell
kubectl get job
```
![[image-245.png]]

pod 상태를 확인합니다.
```shell
kubectl get pod
```
![[image-246.png]]

로그를 확인합니다.
```shell
kubectl logs -f myjob-ccpjg
```

Pod 완료를 확인합니다.
```shell
kubectl get pod
```

Job 완료를 확인합니다.
```shell
kubectl get job
```


Pod 리소스는 계속 Running 상태가 아닌 작업을 마치면 Completed로 남습니다.

**리소스 정리**
```shell
kubectl delete job --all
```

### 7.6.2 CronJob
- CronJob은 Job 리소스와 유사하지만 주기적으로 Job을 실행하는 리소스입니다.

1분마다 date 명령어를 수행하는 cronjob yaml 파일을 작성합니다.
```yaml
# cronjob.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: hello
spec:
  schedule: "*/1 * * * *"
  jobTemplate:
    spec:
      template:
        spec:
          restartPolicy: OnFailure
          containers:
          - name: hello
            image: busybox
            args:
            - /bin/sh
            - -c
            - date; echo Hello from the Kubernetes cluster
```
- schedule : Job 리소스를 실행하는 시간 주기를 설정
- jobTemplate : Job 리소스에서 사용하는 스펙을 동일하게 설정합니다.

```shell
kubectl apply -f cronjob.yaml
```
![[image-247.png]]

cronjob 리소스를 조회합니다.
```shell
kubectl get cronjob
```
![[image-248.png]]

job 리소스를 조회합니다.
```shell
kubectl get job
```
![[image-249.png]]

Pod들을 조회합니다.
```shell
kubectl get pod
```
![[image-250.png]]

Pod의 로그 내용을 확인합니다.
```shell
kubectl logs hello-29006363-4grl8
```
![[image-251.png]]

#### 리소스 정리
```shell
kubectl delete cronjob --all
```

## 7.7 마치며
- 쿠버네티스는 모든 것을 리소스로 표현함
- 빌딩 블럭처럼 작은 단위의 리소스를 조합하여 점점 더 큰 리소스로 만든다. 예를 들어 컨테이너가 실행중이고 해당 컨테이너를 감싸는 Pod가 존재하고, Pod들을 감싸는 Service Deployment가 존재하고 Service Deployment를 감싸는 Application이 존재합니다.
- 다음장에서는 쿠버네티스 패키지 매니저인 helm에 대해서 살펴본다.