
- [[#12.1 리소스 관리|12.1 리소스 관리]]
	- [[#12.1 리소스 관리#12.1.1 LimitRange|12.1.1 LimitRange]]
	- [[#12.1 리소스 관리#12.1.2 ResourceQuota|12.1.2 ResourceQuota]]
- [[#12.2 노드 관리|12.2 노드 관리]]
	- [[#12.2 노드 관리#12.2.1 Cordon|12.2.1 Cordon]]
	- [[#12.2 노드 관리#12.2.2 Uncordon|12.2.2 Uncordon]]
	- [[#12.2 노드 관리#12.2.3 Drain|12.2.3 Drain]]
- [[#12.3 Pod 개수 유지|12.3 Pod 개수 유지]]
- [[#12.4 마치며|12.4 마치며]]


## 12.1 리소스 관리
이번 절에서는 쿠버네티스의 리소스 관리는 담당하는 LimitRange, ResourceQuota 리소스에 대해 각각 살펴봅니다. Pod의 resource property를 통해서 리소스 관리하는 방법을 알아보았습니다. 이번 절에서는 기존의 방법과 LimitRange, ResourceQuota는 어떤 차이점이 있는지 알아봅니다. 해당 차이점을 알기 위해서 쿠버네티스 사용자를 일반 사용자와 관리자로 나누어봅니다.
일반 사용자는 쿠버네티스 플랫폼 위에 자신이 개발한 애플리케이션을 실행하는 사용자입니다. 관리자는 쿠버네티스 클러스터 자체를 관리하고 필요한 물리 리소스(노드, 디스크, 네트워크 등)을 제공하는 총 책임자입니다. 이때 **클러스터 관리자가 일반 사용제게 리소스 사용량을 제한하기 위해 사용하는 것이 LimitRange와 ResourceQuota 리소스 입니다.**

### 12.1.1 LimitRange
LimitRange 기능 2가지
- 일반 사용자가 리소스 사용량 정의를 생략해도 쿠버네티스에서 자동으로 Pod의 리소스 사용량을 설정합니다.
- 관리자가 설정한 최대 요청량을 일반 사용자가 넘지 못하게 제한합니다.

LimitRange 리소스
- 일반 사용자의 Pod 리소스 설정을 통제하는 리소스입니다.

예를 들어 nginx Pod를 생성해봅니다.
```shell
kubectl run mynginx --image nginx
```
![[image-493.png]]

생성한 mynginx Pod의 resource 설정을 조회해봅니다.
```shell
kubectl get pod mynginx -o yaml | grep resources
```
![[image-494.png]]
- 실행 결과를 보면 아무것도 설정되어 있지 않습니다. 이는 **일반 사용자가 생성한 Pod가 노드 전체의 리소스를 고갈시킬 위험이 있습니다.**

위와 같은 리소스 고갈 문제를 예방하기 위해서 클러스터 관리자가 네임스페이스에 LimitRange를 설정합니다.
```yaml
# limit-range.yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: limit-range
spec:
  limits:
  - default:
      cpu: 400m
      memory: 512Mi
    defaultRequest:
      cpu: 300m
      memory: 256Mi
    max:
      cpu: 600m
      memory: 600Mi
    min:
      memory: 200Mi
    type: Container
```
- default
	- 생략 시 사용되는 기본 limit 설정값
- defaultRequest
	- 생략 시 사용되는 기본 request 설정 값
- max
	- 일반 사용자가 요청할 수 있는 최대치를 설정함
- min
	- 일반 사용자가 요청할 수 있는 최소치를 설정함

default와 defaultRequest의 차이
- default : 컨테이너가 리소스 요청 및 제한(limits)을 설정하지 않을 때 적용되는 기본값
- defaultRequest : 컨테이너 리소스 요청(requests)을 설정하지 않을 때 적용되는 기본 요청값
- 예를 들어 컨테이너가 requests 및 limits를 설정하지 않으면 위 예제를 기준으로 requests.cpu=300m, requests.memory=256Mi, limits.cpu=400m, limits.memory=512Mi가 됩니다.

default namespace에 LimitRange 리소스를 설정한 다음에 리소스 설정 없이 Pod를 생성합니다.
```shell
kubectl apply -f limit-range.yaml
```
![[image-496.png]]

```shell
kubectl run nginx-lr --image nginx
```
![[image-497.png]]

nginx-lr Pod의 상세 정보를 조회하여 리소스 정보가 LimitRange 리소스에 따라서 설정되어 있는지 확인해봅니다.
```shell
kubectl get pod nginx-lr -o yaml | grep -A 6 resources
```
![[image-498.png]]
- 실행 결과를 보면 limits, requests 설정이 LimitRange 리소스 설정에 따라서 설정된 것을 볼수 있습니다.

이번에는 사용자가 Pod 설정에서 LimitRange에 벗어난 리소스를 설정해보겠습니다.
```yaml
# pod-exceed
apiVersion: v1
kind: Pod
metadata:
  name: pod-exceed
spec:
  containers:
  - name: nginx
    image: nginx
    resources:
      limits:
        cpu: "700m"
        memory: "700Mi"
      requests:
        cpu: "300m"
        memory: "256Mi"
```
- 위 정의서에서 limits.cpu 및 limits.memory 설정이 LimitRange에서 설정한 리소스보다 높게 설정한 것을 볼수 있습니다.

Pod 리소스를 생성해보겠습니다.
```shell
kubectl apply -f pod-exceed.yaml
```
![[image-499.png]]
- 에러 출력문을 보면 cpu limit이 600m인데, 컨테이너는 700m으로 설정되었고 memory는 600Mi이 최대인데 컨테이너는 700Mi로 설정되었다고 합니다.

위 예제와 같이 LimitRange는 일반 사용자가 애플리케이션 실행시 리소스 설정을 제어하기 위해서 사용한다는 것을 알수 있습니다.

#### clean up
```shell
kubectl delete limitrange limit-range
kubectl delete pod --all
```

### 12.1.2 ResourceQuota
- LimitRange는 개별 Pod 생성에 관여했다면, ResourceQuota(리소스쿼터)는 전체 namespace에 대한 제약을 설정할 수 있는 리소스입니다.
- **클러스터 관리자가 특정한 namespace의 전체 리소스 사용량을 제약하고 싶을 때 ResourceQuota 리소스를 설정합니다.**
- LimitRange 리소스만 사용하면 1개의 Pod를 생성할 때 리소스는 제약할 수 있지만, 이 Pod를 여러개 만들 경우 마찬가지로 클러스터의 리소스를 전부 소진시킬 위험이 있습니다.
- 리소스 고갈 문제를 해결하고자 개별 Pod 리소스 제한만이 아닌 namespace 자체에 Quota를 설정합니다.

다음 예제는 ResourceQuota 리소스 정의서를 구현해서 default namespace에 전체 총합의 limit과 request의 크기를 넘지 못합니다.
```yaml
# res-quota.yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: res-quota
spec:
  hard:
    limits.cpu: 700m
    limits.memory: 800Mi
    requests.cpu: 500m
    requests.memory: 700Mi
```
- limits.cpu
	- namespace의 CPU limit 총합을 제한
- limits.memory
	- namespace의 memory limit 총합을 제한
- requests.cpu
	- namespace의 CPU request 총합을 제한
- requests.memory
	- namespace의 memory request 총합을 제한

default namespace의 Pod 전체 총합이 limit은 CPU의 경우 700m을 넘으면 안되고 메모리는 800Mi을 넘으면 안됩니다. request은 CPU의 경우 500m을 넘으면 안되고, 메모리는 700Mi를 넘으면 안됩니다.

ResourceQuota 리소스를 생성해봅니다.
```yaml
kubectl apply -f res-quota.yaml
```
![[image-500.png]]

limits이 CPU가 700m이 넘지 않는 Pod를 만들어봅니다.
```yaml
# rq-1.yaml
apiVersion: v1
kind: Pod
metadata:
  name: rq-1
spec:
  containers:
  - image: nginx
    name: nginx
    resources:
      limits:
        cpu: "600m"
        memory: "600Mi"
      requests:
        cpu: "300m"
        memory: "300Mi"
```

rq-1 Pod를 생성해봅니다.
```shell
kubectl apply -f rq-1.yaml
```
![[image-501.png]]

rq-1 Pod를 생성한 다음에 다시 똑같은 스펙의 Pod를 생성해봅니다. rq-2.yaml 파일에서 metadata.name을 rq-2로 변경해야 합니다.
```shell
cp rq-1.yaml rq-2.yaml
```
```yaml
# rq-2.yaml
apiVersion: v1
kind: Pod
metadata:
  name: rq-2
spec:
  containers:
  - image: nginx
    name: nginx
    resources:
      limits:
        cpu: "600m"
        memory: "600Mi"
      requests:
        cpu: "300m"
        memory: "300Mi"
```

rq-2 Pod를 생성해봅니다.
```shell
kubectl apply -f rq-2.yaml
```
![[image-502.png]]
- 실행 결과를 보면 res-quota 리소스에 의해서 리소스 사용량이 제한되었다는 메시지를 받습니다.
- 일반 사용자가 Pod 생성에 요청한 limit의 cpu 및 memory는 600m, 600Mi, request의 cpu 및 memory는 300m 이었지만, limits.cpu, limits.memory, requests.cpu가 총합을 넘어버려서 제한되었다는 메시지입니다.

이와 같이 Limit Range, Resource Quota 리소스를 이용하면 각 Pod에 할당되는 리소스 사용량을 제한할 수도 있고 네임스페이스 단위로 리소스 총합 사용량을 제한할 수 있습니다.

#### clean up
```shell
kubectl delete resourcequota res-quota
kubectl delete pod --all
```

## 12.2 노드 관리
- 쿠버네티스로 서비스 운영시 리소스 관리만이 아니라 노드 자체에 대한 관리가 필요한 경우가 있습니다.
- 온프레미스 환경에서 물리적인 디스크의 손상, 내부 네트워크 장애가 있을 수 있습니다. 클라우드 서비스인 경우에는 서버 타입 변경, 디스크 교체 등으로 인해 노드를 일시적으로 중단하고 관리해야 하는 경우가 있습니다.
- 위와 같은 상황을 해결하기 위해서 **쿠버네티스에서는 노드를 유지보수 상태로 전환하여 더는 새로운 Pod가 스케줄링 되지 않게 설정할 수 있습니다.**

**노드 관리 명령어**
- Cordon
	- 노드를 유지보수 모드로 전환
- Uncordon
	- 유지보수가 완료된 노드를 다시 정상화시킴
- Drain
	- 노드를 유지보수 모드로 전환하되, 기존의 Pod들을 퇴거(Evict)시킵니다.

### 12.2.1 Cordon
명령어 형식
```
kubectl cordon {NODE}
```
- 특정 노드를 유지보수 모드로 전환하기 위해서 cordon이라는 명령어를 사용함
- 해당 명령어를 사용하면 해당 노드에 더이상 Pod가 스케줄링되지 않습니다.

worker 노드를 유지보수 상태로 만들기 전에 상태를 확인해봅니다.
```shell
kubectl get node worker -o yaml | grep spec -A 5
```
![[image-503.png]]
- 실행 상태를 보면 별도 상태 출력없이 정상적으로 동작하고 있는 것을 볼수 있습니다.

worker 노드를 유지보수 상태로 만들어보겠습니다.
```shell
kubectl cordon worker
```
![[image-504.png]]

다시 worker 상태를 확인해봅니다.
```shell
kubectl get node worker -o yaml | grep spec -A 10
```
![[image-505.png]]
- 실행 결과를 보면 taint가 설정된 것을 볼수 있고, unscheduleable=true로 설정된 것을 볼수 있습니다. 이는 worker 노드가 Pod를 스케줄링 할 수 없는 유지보수 상태인것을 알수 있습니다.

worker 노드의 상태를 간단히 봅니다.
```shell
kubectl get node
```
![[image-506.png]]
- 실행 결과를 보면 worker 노드의 Status가 Ready만이 아닌 SchedulingDisabled 또한 생성된 것을 볼수 있습니다.

그러면 Pod를 생성할 때 정말로 worker 노드에 스케줄링되지 않는지 확인해보겠습니다. 다음 yaml 정의서는 nginx Pod를 5개 생성하는 ReplicaSet 리소스입니다.
```yaml
# rs.yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: rs
spec:
  replicas: 5
  selector:
    matchLabels:
      run: rs
  template:
    metadata:
      labels:
        run: rs
    spec:
      containers:
      - name: nginx
        image: nginx
```

ReplicaSet 리소스를 생성해봅니다.
```shell
kubectl apply -f rs.yaml
```
![[image-507.png]]

생성된 Pod들이 전부 master 노드에 스케줄링되었는지 확인해봅니다.
```shell
kubectl get pod -o wide
```
![[image-508.png]]
- 실행 결과를 보면 worker 노드가 아닌 전부 master 노드에 할당된 것을 볼수 있습니다. 왜냐하면 현재 worker 노드는 유지보수 상태이기 때문에 Pod를 스케줄링 할 수 없기 때문입니다.

이번에는 Pod 리소스를 정의할 때 nodeSelector에 명시적으로 worker 노드에서 실행하라고 구현해보겠습니다.
```yaml
# pod-worker.yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-worker
spec:
  containers:
  - image: nginx
    name: nginx
  nodeSelector:
    kubernetes.io/hostname: worker
```

pod-worker Pod를 생성해봅니다.
```shell
kubectl apply -f pod-worker.yaml
```
![[image-509.png]]

worker 노드에 pod가 스케줄링되었는지 상태를 확인해봅니다.
```shell
kubectl get pod -o wide
```
![[Pasted image 20250318142550.png]]
- 실행 결과를 보면 pod-worker Pod의 상태가 Pending 상태인 것을 볼수 있습니다.
- Pending 상태를 유지하는 이유는 현재 worker 노드가 유지보수 모드이기 때문입니다.

### 12.2.2 Uncordon
uncordon 명령어는 유지보수 모드 상태인 노드를 다시 되돌리는 명령어입니다.

worker 노드를 유지보수 모드 상태에서 Pod 스케줄링 가능 상태로 되돌려보겠습니다.
```shell
kubectl uncordon worker
```
![[image-511.png]]

worker 노드의 상세 정보를 조회해서 어떤 상태인지 확인해봅니다.
```shell
kubectl get node worker -o yaml | grep spec -A 10
```
![[image-512.png]]
- 실행 결과를 보면 taint와 unscheduleable=true 설정이 사라진 것을 볼수 있습니다.

간단하게 node 상태도 확인해보겠습니다.
```shell
kubectl get node
```
![[image-513.png]]
- 기존 worker 노드에 존재하던 UnScheduleable 상태도 사라진 것을 볼 수 있습니다.

이전 예제에서 실행한 pod-worker Pod도 worker 노드에 스케줄링되었는지 확인해봅니다.
```shell
kubectl get pod -o wide
```
![[image-514.png]]
- 실행 결과를 보면 정상적으로 worker 노드에 스케줄링 된것을 볼수 있습니다.

pod-worker Pod를 삭제합니다.
```shell
kubectl delete pod pod-worker
```
![[image-515.png]]

### 12.2.3 Drain
노드를 cordon 명령어를 이용하여 유지보수 모드 상태로 만들면 새로운 Pod가 스케줄링되지는 않지만, 기존 실행중인 Pod에 대해서는 관여하지 않습니다.
기존에 실행중인 Pod들도 퇴거(Evict)시키기 위해서는 drain 명령을 사용해야 합니다. drain 명령을 사용하면 Pod가 더는 할당되지 않게 taint 시킬뿐만 아니라 기존의 Pod들도 퇴거시킵니다.

예제를 위해서 Deployment 리소스를 정의합니다.
```yaml
# nginx.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 3
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
```

nginx Deployment 리소스를 생성해봅니다.
```shell
kubectl apply -f nginx.yaml
```
![[image-516.png]]

worker 노드에 Pod가 스케줄링되었는지 확인해봅니다.
```shell
kubectl get pod -o wide
```
![[image-517.png]]
- 실행 결과를 보면 한개의 nginx Pod가 worker 노드에서 실행되고 있는 것을 볼수 있습니다.

drain 명령어를 이용하여 worker 노드를 유지보수 모드 상태로 만들고 해당 노드에서 실행중인 Pod를 퇴거시켜보겠습니다.
```shell
kubectl drain worker --ignore-daemonsets
```
- DaemonSet 리소스로 배포된 Pod는 기본적으로 kubectl drain 명령어로 제거되지 않기 때문에 옵션(--ignore-daemonsets)을 사용해서 제거합니다.
![[image-518.png]]
- 실행 결과를 보면 nginx Pod가 퇴거되었다고 메시지를 출력합니다.

nginx Pod가 어떻게 동작하고 있는지 확인해봅니다.
```shell
kubectl get pod -o wide
```
![[image-519.png]]
- 실행 결과를 보면 worker 노드에서 퇴거되어 master 노드에서 실행중인 것을 볼수 있습니다.

이번에는 worker 노드의 상태를 조회해봅니다.
```shell
kubectl get node worker -o yaml | grep spec -A 10
```
![[image-520.png]]
- 실행 결과를 보면 worker 노드에 taint와 unschedulable 추가 된것을 볼수 있습니다.

간단한 노드 상태도 봅니다.
```shell
kubectl get node
```
![[image-521.png]]
- 실행 결과를 보면 worker 노드의 상태에 SchedulingDisabled 상태가 추가된 것을 볼수 있습니다.

darin된 노드를 다시 uncordon 명령을 이용해서 다시 되돌립니다.
```shell
kubectl uncordon worker
```
![[image-522.png]]

## 12.3 Pod 개수 유지
 drain 명령어를 사용하여 노드가 유지보수 모드 상태로 전환되면, 해당 노드에서 실행중인 Pod가 종료됩니다. 하지만 Deployment 리소스가 곧바로 새로운 Pod를 생성해주지만, 일시적으로 Pod의 개수가 급격히 줄어듭니다. 
 만약 트래픽을 많이 서비스를 운영중이라면 순간적으로 모든 부하가 한쪽 Pod에게 쏠려서 응답 지연이 발생할 수 있습니다.
 위와 같은 문제를 해결하기 위해서 PodDistruptionBudget 리소스를 사용할 수 있습니다. **해당 리소스는 운영중인 Pod의 개수를 항상 일정 수준으로 유지할 수 있도록 Pod의 퇴거를 막아주는 역할을 수행합니다.**

쿠버네티스에서 클러스터 관리자가 의도(예를 들어 유지보수 목적)를 가지고 Pod를 중단하는 것을 Voluntary Disruptions(자진 중단)이라고 부릅니다. 장애로 인해서 갑자기 Pod가 중단된 것이 아니기 때문에 사전에 알수 있습니다. pdb 리소스는 노드 유지보수 작업을 위해서 자진 중단한 상황에서 Pod의 개수가 급격히 내려가지 않게 막아줍니다.

예를 들어 이전 예제에서 구현한 nginx deployment 리소스의 replica를 10으로 올려봅니다.
```shell
kubectl scale deploy nginx --replicas 10
```
![[image-523.png]]

다음 pdb 리소스를 정의합니다.
```yaml
# nginx-pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: nginx-pdb
spec:
  minAvailable: 9
  selector:
    matchLabels:
      app: nginx
```
- minAvailable : 최소 유지해야 하는 Pod 개수
- selector: 유지할 Pod 선택

nginx-pdb 리소스를 생성해봅니다.
```shell
kubectl apply -f nginx-pdb.yaml
```
![[image-524.png]]

이제 worker 노드를 drain하여 유지보수 모드 상태로 전환해봅니다.
```shell
kubectl drain worker --ignore-daemonsets
```
![[image-525.png]]
- 실행 결과를 보면 10개의 Pod 중 최소 9개의 Pod가 유지되어야 해서 1개씩 Pod를 퇴거하고 다른 노드에 새로 생성을 합니다.

위와 같이 pdb 리소스를 사용하면 노드를 유지보수 모드로 변경해도 Pod의 개수를 안정적으로 감소시킬수 있습니다.

## 12.4 마치며
- 클러스터 관리자 입장에서 클러스터를 안정적으로 운영하고 유지보수하는 방법에 대해서 알아봄
- 사용자의 리소스 사용량을 관리하여 리소스 고갈 발생을 예방하는 방법을 알아봄
- 서버를 안전한 방법으로 유지보수 상태로 전환하는 방법에 대해서 알아봄
- 다음장에서는 쿠버네티스의 접근 제어 메커니즘에 대해서 알아봄

#### clean up
```shell
kubectl delete pdb nginx-pdb
kubectl delete deploy nginx
kubectl delete rs rs
kubectl uncordon worker
```