
- [[#11.1 고가용성 확보 - Pod 레벨|11.1 고가용성 확보 - Pod 레벨]]
	- [[#11.1 고가용성 확보 - Pod 레벨#11.1.1 metrics server 설치|11.1.1 metrics server 설치]]
	- [[#11.1 고가용성 확보 - Pod 레벨#11.1.2 자동 확장할 Pod 생성|11.1.2 자동 확장할 Pod 생성]]
	- [[#11.1 고가용성 확보 - Pod 레벨#11.1.3 hpa 생성 - 선언형 명령|11.1.3 hpa 생성 - 선언형 명령]]
	- [[#11.1 고가용성 확보 - Pod 레벨#11.1.4 hpa 생성 - 명령형 명령|11.1.4 hpa 생성 - 명령형 명령]]
- [[#11.2 고가용성 확보 - Node 레벨|11.2 고가용성 확보 - Node 레벨]]
- [[#11.3 Taint & Toleration|11.3 Taint & Toleration]]
	- [[#11.3 Taint & Toleration#11.3.1 Taint|11.3.1 Taint]]
	- [[#11.3 Taint & Toleration#11.3.2 Toleration|11.3.2 Toleration]]
- [[#11.4 Affinity & AntiAffinity|11.4 Affinity & AntiAffinity]]
	- [[#11.4 Affinity & AntiAffinity#11.4.1 NodeAffinity|11.4.1 NodeAffinity]]
	- [[#11.4 Affinity & AntiAffinity#11.4.2 PodAffinity|11.4.2 PodAffinity]]
	- [[#11.4 Affinity & AntiAffinity#11.4.3 PodAntiAffinity|11.4.3 PodAntiAffinity]]
	- [[#11.4 Affinity & AntiAffinity#11.4.4 PodAffinity와 PodAntiAffinity 활용법|11.4.4 PodAffinity와 PodAntiAffinity 활용법]]
- [[#11.5 마치며|11.5 마치며]]


## 11.1 고가용성 확보 - Pod 레벨
ReplicaSet, Deployment 리소스의 replica property를 이용하여 서비스의 가용성을 높이는 방법을 알아보았습니다. 이 방법은 일정 범위 안의 트래픽에 대해서는 서비스 가용성을 유지하지만, **처리량 범위를 넘어서는 트래픽에 대해서는 한계를 가집니다. 왜냐하면 replica 개수가 정적으로 고정되어 있기 때문입니다.**

위와 같은 문제를 해결하기 위해서 쿠버네티스에서는 Pod의 리소스 사용량에 따라서 자동으로 확장하는 **HorizontalPodAutoScaler(hpa)**라는 리소스를 제공합니다. hpa 리소스는 Pod의 개수를 스케일 아웃(scale-out, Pod의 개수를 수평 확장)합니다.


> [!NOTE] 용어 정리
> 수평적 확장: 동일한 프로세스 또는 노드의 개수를 늘리는 것을 말합니다. scale-out이라고도 말합니다.
> 수직적 확장: 프로세스 또는 노드의 성능을 높히는 것을 의미합니다. scale-up이라고도 합니다.

HPA 수행과정
1. HPA는 Mrtric Server(Pod 리소스 사용량 모니터링하는 서버)에게 리소스 사용량 질의합니다.
2. HPA는 ReplicaSet 리소스의 replicas의 개수를 조절합니다.

### 11.1.1 metrics server 설치
hpa 사용을 위해서 metrics-server를 먼저 설치해봅니다. k3s 설치시 우리는 metrics-server를 비활성화하였습니다. 이번에는 재설치하여 metrics-server를 활성해봅니다.
```shell
curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC="
     --disable traefik
     --node-name master --docker" \
     sh -s -
```

kube-system namespace에서 실행중인 metrics-server pod를 확인합니다.
```shell
kubectl get pods -n kube-system
```
![[image-461.png]]

mynginx Pod를 생성해봅니다.
```shell
kubectl run mynginx --image nginx
```

Pod별 리소스 사용량을 확인해봅니다.
```shell
kubectl top pod
```
![[image-462.png]]
- 위 실행 결과를 보면 13MB를 사용하고 있는 것을 볼수 있습니다.

이번에는 Node별 리소스 사용량을 확인해봅니다.
```shell
kubectl top node
```
![[image-463.png]]

mynginx Pod를 삭제합니다.
```shell
kubectl delete pod mynginx
```

metrics-server가 정상적으로 설치되면 top 명령을 이용하여 Pod 및 Node 리소스 사용량을 모니터링할 수 있습니다.

### 11.1.2 자동 확장할 Pod 생성
다음과 같이 Deployment와 Service 리소스에 대한 정의서를 구현합니다.
```yaml
# heavy-cal.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: heavy-cal
spec:
  selector:
    matchLabels:
      run: heavy-cal
  replicas: 1
  template:
    metadata:
      labels:
        run: heavy-cal
    spec:
      containers:
      - name: heavy-cal
        image: k8s.gcr.io/hpa-example
        ports:
        - containerPort: 80
        resources:
          limits:
            cpu: 500m
          requests:
            cpu: 300m
---
apiVersion: v1
kind: Service
metadata:
  name: heavy-cal
spec:
  ports:
  - port: 80
  selector:
    run: heavy-cal
```
- hpa가 정상 동작하기 위해서는 반드시 requests property가 정의되어야 합니다.

위에서 작성한 정의서를 기반으로 리소스를 생성합니다.
```shell
kubectl apply -f heavy-cal.yaml
```
![[image-464.png]]

### 11.1.3 hpa 생성 - 선언형 명령
다음과 같이 HorizontalPodAutoscaler 리소스를 생성합니다.
```yaml
# hpa.yaml
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: heavy-cal
spec:
  maxReplicas: 50
  minReplicas: 1
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: heavy-cal
  targetCPUUtilizationPercentage: 50
```
- maxReplicas
	- replica 최대 개수 설정
- minReplicas
	- replica 최소 개수 설정
- scaleTargetRef
	- 모니터링할 타겟 설정
- targetCPUUtilizationPercentage
	- 리소스의 임계치를 설정

hpa 리소스는 heavy-cal Deployment를 모니터링하고 있다가 CPU 리소스가 50%가 넘었을 때 최대 50개까지 replica 개수를 증가시키는 역할을 수행합니다.

위 정의서를 기반으로 hpa 리소스를 생성합니다.
```shell
kubectl apply -f hpa.yaml
```
![[image-465.png]]

### 11.1.4 hpa 생성 - 명령형 명령
다음 명령어를 이요해서 앞서 생성한 hpa를 동일하게 생성할 수 있습니다.
```shell
kubectl autoscale deployment heavy-cal \
	--cpu-percent=50
	--min=1
	--max=50
```

hpa 리소스가 생성되었는지 확인합니다.
```shell
kubectl get hpa
```
![[image-466.png]]

heavy-cal 서비스에 무한히 부하를 주는 Pod를 생성합니다.
```yaml
# heavy-load.yaml
apiVersion: v1
kind: Pod
metadata:
  name: heavy-load
spec:
  containers:
  - name: busybox
    image: busybox
    command: ["/bin/sh"]
    args: ["-c", "while true; do wget -q -O- http://heavy-cal; done"]
```

Pod 리소스를 생성하여 서비스에 부하를 주어보겠습니다.
```shell
kubectl apply -f heavy-load.yaml
```
![[image-467.png]]

watch 명령어를 이용해서 heavy-cal Pod의 개수 증가를 확인해봅니다.
```shell
watch kubectl top pod
```
![[image-468.png]]
- 실행 결과를 보면 heavy-cal Pod가 계속 증가하는 것을 볼수 있습니다.

부하를 주는 Pod를 삭제합니다.
```shell
kubectl delete pod heavy-load
```

## 11.2 고가용성 확보 - Node 레벨
- 쿠버네티스에서는 Pod만이 아닌 Cluster 레벨의 Node 또한 자동적으로 수평 확장할 수 있는 방식을 제공합니다.
- Node에 더이상 새로운 Pod 리소스를 할당할 수 없는 경우, 자동으로 새로운 Node를 생성해서 클러스터 레벨의 고가용성을 확보할 수 있습니다.
- Cluster AutoScale의 경우에는 클라우드 서비스에서 지원해야 하기 때문에 k3s 클러스터로는 따라하기 힘듭니다.

## 11.3 Taint & Toleration
Pod와 Node 간 특성에 따라서 또는 Pod 끼리의 특성에 따라서 스케줄링 전략을 세울수 있는 방법에 대해 살펴봅니다.

### 11.3.1 Taint
- Taint는 Node 리소스에 적용하는 설정값입니다.
- 노드에 Taint 설정을 하게 되면 Pod들이 해당 노드에 스케줄링되지 못합니다.

Taint 명령어 형식
```shell
kubectl taint nodes $NODE_NAME <KEY>=<VALUE>:<EFFECT>
```
- key: taint의 키값 설정, 문자열을 설정합니다.
- value: taint의 value 값 설정, 문자열을 설정합니다.
- effect: NoSchedule, PerferNoSchedule, NoExecute 종류 중 하나를 선택합니다.

effect 종류에 따른 스케줄링 정책
- PreferNoSchedule
	- taint된 노드에 새로운 Pod를 스케줄링하는 것을 지양합니다.
	- 다른 노드에 먼저 스케줄링하고 더이상 다른 노드의 가용한 리소스가 없는 경우 PreferNoScheduel이 설정된 노드에 스케줄링됩니다.
- NoSchedule
	- taint된 노드에 새로운 Pod를 스케줄링하지 못하게 막습니다.
	- 기존 실행하고 있는 Pod에 대해서는 영향이 없습니다.
- NoExecute
	- taint된 노드에 새로운 Pod를 스케줄링하지 못하게 막을 뿐만 아니라 기존에 돌고 있던 Pod들도 삭제합니다.
	- 가장 강한 정책

### 11.3.2 Toleration
- toleration은 Pod에 적용하는 설정값입니다.
- Taint된 노드라 해도 Pod가 tolerate(참을수 있으면) Taint된 노드에 Pod를 스케줄링 될수 있습니다.

예를 들어 어떤 노드에 NoSchedule이라는 Taint가 적용되어도 Pod에 NoSchedule에 대한 Toleration이 적용된 경우 해당 노드에 스케줄링될 수 있습니다.

워커 노드에는 프로젝트 A와 관련된 Pod만 스케줄링하려고 합니다. 이를 프로젝트 A 전용 서버라고 하겠습니다.
project=A라는 taint를 설정하며, effect 종류로는 NoSchedule를 설정합니다.
```shell
kubectl taint node worker project=A:NoSchedule
```
![[image-469.png]]

worker 노드에 taint가 설정되었는지 확인합니다.
```shell
kubectl get node worker -o yaml | grep -A 4 taints
```
![[image-470.png]]

no-tolerate Pod를 생성하여 스케줄링해봅니다.
```yaml
# no-tolerate.yaml
apiVersion: v1
kind: Pod
metadata:
  name: no-tolerate
spec:
  containers:
  - name: nginx
    image: nginx
```

no-tolerate Pod를 생성해봅니다.
```shell
kubectl apply -f no-tolerate.yaml
```
![[image-472.png]]

마스터 노드에 스케줄링 되었는지 확인합니다.
```shell
kubectl get pod -o wide
```
![[image-473.png]]

이번에는 Pod에 Toleration 설정을 적용해보겠습니다.
```yaml
# tolerate.yaml
apiVersion: v1
kind: Pod
metadata:
  name: tolerate
spec:
  containers:
  - name: nginx
    image: nginx
  tolerations:
  - key: "project"
    value: "A"
    operator: "Equal"
    effect: "NoSchedule"
```
- tolerations
	- key: toleration의 key값 설정
	- value: taint된 값에 대해서 tolerate하겠다는 의미입니다.
	- operator: Equal, Exists 중 하나를 선택합니다.
		- Equal: key, value가 항상 동일해야 하고, Exists인 경우 key의 값만 동일하면 tolerate합니다.
	- effect: taint에 적용된 effect에 대해서 tolerate 하겠다는 것을 의미합니다.

Pod 리소스를 생성해봅니다.
```shell
kubectl apply -f tolerate.yaml
```
![[image-474.png]]

Pod 리소스 생성되었는지 확인합니다.
```shell
kubectl get pod -o wide
```
![[image-476.png]]
- 실행 결과를 보면 worker 노드에 Taint(project=A:NoSchdule)가 설정되었음에도 tolerate Pod가 worker 노드에서 실행된 것을 볼수 있습니다.
- taint와 toleration은 key=value 형식이기 때문에 1개 이상 적용이 가능합니다.
	- 현재는 worker 노드에 project=A:NoSchdule taint가 설정되어 있는 상태입니다.
![[image-477.png]]

이번에는 worker 노드에 key만 종재하는 taint를 적용해봅니다.
```shell
kubectl taint node worker badsector=:NoSchedule
```
![[image-478.png]]

worker 노드에 taint 설정이 잘되엇는지 확인해봅니다.
```shell
kubectl get node worker -o yaml | grep -A 7 taints
```
![[image-479.png]]
- 위 실행 결과를 보면 badselector만 존재하고 value는 존재하지 않은 taint가 설정된 것을 볼수 있습니다.

이번에는 badsector taint를 이용하여 Pod를 생성해봅니다.
```yaml
# badsector.yaml
apiVersion: v1
kind: Pod
metadata:
  name: badsector
spec:
  containers:
  - name: nginx
    image: nginx
  tolerations:
  - key: "project"
    value: "A"
    operator: "Equal"
    effect: "NoSchedule"
  - key: "badsector"
    operator: "Exists"
```

badsector Pod를 생성해봅니다.
```shell
kubectl apply -f badsector.yaml
```
![[image-480.png]]

badsector Pod가 worker 노드에 스케줄링되었는지 확인합니다.
```shell
kubectl get pod -o wide
```
![[image-481.png]]
- 실행 결과를 보면 worker node에 NoSchedule이 설정되었음에도 worker 노드에 성공적으로 스케줄링된 것을 볼수 있습니다.

taint한 노드를 되돌리기 위해서 다음과 같이 실행합니다.
```shell
kubectl taint node worker project-
kubectl taint node worker badsector-
kubectl delete pod --all
```
![[image-482.png]]

이번절에서 taint와 toleration을 학습하였습니다. taint와 toleration을 안정적인 서비스를 운영하는데 활용할 수 있습니다. 특정 노드에 대해서 보수 작업을 하거나, 네트워크 이상이 생긴다거나 노드가 특정 상태에 처해있을 때 해당 노드를 taint시켜서 쿠버네티스에게 알리면 노드의 상태를 충분히 인지하고 감안 할 수있는 Pod에 대해서만 스케줄링하기 때문에 사용자 입장에서 세밀한 Pod 스케줄링을 수행할 수 있습니다.

## 11.4 Affinity & AntiAffinity
Affinity와 AntiAffintiy는 특정 Node나 Pod와의 거리를 조절하는데 사용됩니다.
예를 들어 **특정 Pod끼리 서로 가까이 스케줄링되고 싶은 경우, Affinity(친밀함)을 사용하고, 반대로 서로 멀리 스케줄링 되고 싶은 경우 AntiAffinity(반-친밀함)을 사용**합니다.
- Affinity
	- NodeAffinity
		- 특정 Node와 가까이 할당되기 원할 때 사용
	- PodAffinity
		- 특정 Pod 끼리 가까이 할당되기 원할 때 사용
- AntiAffinity
	- PodAntiAffinity
		- 특정 Pod끼리 멀리 할당되기 원할 때 사용

### 11.4.1 NodeAffinity
Pod가 특정 노드에 할당되길 원할 때 사용합니다. nodeSelector와 유사하지만 조금더 상세하게 설정이 가능합니다.
```yaml
# node-affinity.yaml
apiVersion: v1
kind: Pod
metadata:
  name: node-affinity
spec:
  containers:
  - name: nginx
    image: nginx
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: disktype
            operator: In
            values:
            - ssd
```
- nodeAffinity
	- affinity 종류를 선택합니다. 이 예제에서는 nodeAffinity를 선택합니다.
- requiredDuringSchedulingIgnoredDuringExecution
	- 특정 노드에 스케줄링 되길 강제(required)합니다.
- preferredDuringSchedulingIgnoredDuringExecution
	- 특정 노드에 스케줄링 되길 희망(preferred)합니다.
- nodeSelectorTerms
	- 선택할 노드의 라벨 정보를 나타냅니다.
- matchExpressions
	- toleration과 마찬가지로, key, value, operator등을 설정합니다.
- operator
	- 단순한 라벨 매칭이 아닌 In, NotIn, Exists, DoesNotExist, Gt, Lt과 같이 다양한 매칭 정책을 선택할 수 있습니다.
		- `In` / `NotIn` → 특정 값 리스트 포함 여부
		- `Exists` / `DoesNotExist` → 특정 키 존재 여부
		- `Gt` / `Lt` → 특정 숫자 크기 비교


위 Pod를 생성하기 전에 master 노드에 disktype=ssd을 추가하겠습니다.
```shell
kubectl label nodes master disktype=ssd
```
![[image-484.png]]

```shell
kubectl get node master -o yaml | grep "disktype: ssd"
```
![[image-483.png]]

```shell
kubectl apply -f node-affinity.yaml
```
![[image-485.png]]

```
kubectl get pod -o wide
```
![[image-486.png]]
- 실행 결과를 보면 disktype=ssd 레이블을 가지고 있는 master 노드에 할당된 것을 볼수 있습니다.

### 11.4.2 PodAffinity
PodAffinity는 Pod 간의 스케줄링에 관여합니다. matchExpressions이 매칭되는 Pod가 동일한 노드(또는 노드 그룹)에서 실행될 수 있도록 요청합니다.
```yaml
# pod-affinity.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: pod-affinity
spec:
  selector:
    matchLabels:
      app: affinity
  replicas: 2
  template:
    metadata:
      labels:
        app: affinity
    spec:
      containers:
      - name: nginx
        image: nginx
      affinity:
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - affinity
            topologyKey: "kubernetes.io/hostname"
```
- podAffinity
	- nodeAffinity가 아닌 podAffinity로 설정합니다.
- labelSelector
	- 동일한 라벨을 가진 Pod끼리 묶을때 사용합니다.
- topologyKey
	- Pod끼리 묶을 기준을 정의합니다.


> [!NOTE] topologyKey
> 노드 라벨 중 하나를 사용합니다. 단순 노드를 기준으로 묶을 수도 있지만 사용자가 정의한 노드 라벨(topologyKey)을 기준으로 묶을 수도 있습니다. 예시에서는 kubernetes.io/hostname 라벨로 각 노드 기준을 묶지만 노드 그룹을 만들어 묶을 수도 있습니다.(disktype이 ssd인 노드 그룹, hdd인 노드 그룹)

Deployment 리소스를 생성해봅니다.
```shell
kubectl apply -f pod-affinity.yaml
```
![[image-487.png]]

Pod가 생성되어 master 또는 worker 노드 한쪽에 Pod들이 전부 생성되었는지 확인합니다.
```shell
kubectl get pod -o wide
```
![[image-488.png]]
- 실행 결과를 보면 pod-affinity Pod들이 전부 master 노드에서 실행중인 것을 볼수 있습니다. 이는 PodAffinity 기능으로 묶여서 스케줄링 되었기 때문입니다.

### 11.4.3 PodAntiAffinity
이번에는 반대로 Pod끼리 서로 다른 노드에 스케줄링하고 싶을 때 PodAntiAffinity를 사용합니다.

```yaml
# pod-antiaffinity.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: pod-antiaffinity
spec:
  selector:
    matchLabels:
      app: antiaffinity
  replicas: 2
  template:
    metadata:
      labels:
        app: antiaffinity
    spec:
      containers:
      - name: nginx
        image: nginx
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                  - antiaffinity
            topologyKey: "kubernetes.io/hostname"
```
- podAntiAffinity
	- Pod끼리 서로 다른 topology에 스케줄링됩니다.
- labelSelector
	- 서로 다른 곳에 스케줄링될 Pod를 선택합니다.
- topologyKey
	- Pod끼리 서로 다른 곳에 스케줄링될 기준을 설정합니다.

Deployment 리소스를 생성합니다.
```shell
kubectl apply -f pod-antiaffinity.yaml
```
![[image-489.png]]

Pod 리소스를 조회하여 생성된 Pod들이 master 노드와 worker 노드 각각에 생성되었는지 확인해봅니다.
```shell
kubectl get pod -o wide
```
![[image-490.png]]
- 실행 결과를 보면 pod-antiaffinity Pod가 서로 다른 노드인 master, worker 노드에 각각 생성된 것을 볼수 있습니다.

### 11.4.4 PodAffinity와 PodAntiAffinity 활용법
예를 들어 어떤 웹 서비스를 운영한다고 가정합니다. 웹 서비스 뒤에는 cache 서버를 두고 필요한 데이터를 cache 서버에서 꺼내서 사용한다고 가정합니다. 서비스 가용성을 이용해서 1개의 노드가 죽어도 다른 노드에서 서비스를 지속할 수 있도록 web 서버끼리, cache 서버끼리 최대한 서로 다른 노드에 스케줄링되도록 설정하고 싶습니다. 또한 각각의 web 서버와 cache 서버는 네트워크 지연시간을 최소화하기 위해서 최대한 서로 같은 노드에서 실행되도록 설정하고 싶습니다.

즉, 3개의 노드가 존재하고 각각의 노드에는 web 서버와 cache 서버가 세트로 각각 있다고 생각할 수 있습니다.

#### cache 서버 설정
```yaml
# redis-cache.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-cache
spec:
  selector:
    matchLabels:
      app: store
  replicas: 2
  template:
    metadata:
      labels:
        app: store
    spec:
      affinity:
        # cache 서버끼리 멀리 스케줄링
        # app=store 라벨을 가진 Pod끼리 멀리 스케줄링
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - store
            topologyKey: "kubernetes.io/hostname"
      containers:
      - name: redis-server
        image: redis
```

#### web 서버 설정
```yaml
# web-server.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-server
spec:
  selector:
    matchLabels:
      app: web-store
  replicas: 2
  template:
    metadata:
      labels:
        app: web-store
    spec:
      affinity:
        # web 서버끼리 멀리 스케줄링
        # app=web-store 라벨을 가진 Pod끼리 멀리 스케줄링
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - web-store
            topologyKey: "kubernetes.io/hostname"
        # web-cache 서버끼리 가까이 스케줄링
        # app=store 라벨을 가진 Pod끼리 가까이 스케줄링
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
                - key: app
                  operator: In
                  values:
                  - store
            topologyKey: "kubernetes.io/hostname"
      containers:
      - name: web-app
        image: nginx
```

redis-cache.yaml과 web-server.yaml 파일을 이용해서 두개의 Deployment 리소스를 생성합니다.
```shell
kubectl apply -f redis-cache.yaml
kubectl apply -f web-server.yaml
```
![[Pasted image 20250318121731.png]]

web-server Pod와 redis-cache Pod가 master, worker 노드에 각각 1개씩 스케줄링되었는지 확인해봅니다.
```shell
kubectl get pods -o wide
```
![[image-492.png]]
- 실행 결과를 보면 각각의 노드에 Pod들이 스케줄링 된 것을 볼수 있습니다.

## 11.5 마치며
- 서비스를 안정적으로 운영하기 위한 고급 스케줄링 방법을 알아봄
- taint와 toleration을 통해서 노드의 상태에 따라서 Pod를 스케줄링하는 방법에 대해서 알아봄
- Pod 및 Node끼리 서로 가까이 멀리 스케줄링하는 방법에 대해 알아봄
- 고급 스케줄링 방법들은 트래픽이 급격히 늘어나거나 노드나 Pod에 문제가 생기는 등의 갑작스러운 상황이 발생했을 때 사용할 수 있습니다.

#### clean up
```shell
kubectl delete deploy --all
kubectl delete pod --all
```